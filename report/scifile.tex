% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}

\usepackage{scicite}
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font
\usepackage[backend=bibtex,sorting=none, backref=true]{biblatex}
\usepackage{times}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{mathtools}
\usepackage{listings,lstautogobble}
\usepackage{xcolor}
\usepackage{float}

\definecolor{gray}{gray}{0.5}
\colorlet{commentcolour}{green!50!black}

\colorlet{stringcolour}{red!60!black}
\colorlet{keywordcolour}{blue}
\colorlet{exceptioncolour}{yellow!50!red}
\colorlet{commandcolour}{magenta!90!black}
\colorlet{numpycolour}{blue!60!green}
\colorlet{literatecolour}{magenta!90!black}
\colorlet{promptcolour}{green!50!black}
\colorlet{specmethodcolour}{violet}

\newcommand*{\framemargin}{3ex}

\newcommand*{\literatecolour}{\textcolor{literatecolour}}

\newcommand*{\pythonprompt}{\textcolor{promptcolour}{{>}{>}{>}}}

\lstdefinestyle{python}{
	language=python,
	showtabs=true,
	tab=,
	tabsize=4,
	basicstyle=\ttfamily\footnotesize,
	stringstyle=\color{stringcolour},
	showstringspaces=false,
	keywordstyle=\color{keywordcolour}\bfseries,
	emph={as,and,break,class,continue,def,yield,del,elif ,else,%
		except,exec,finally,for,from,global,if,in,%
		lambda,not,or,pass,print,raise,return,try,while,assert,with},
	emphstyle=\color{blue}\bfseries,
	emph={[2]True, False, None},
	emphstyle=[3]\color{commandcolour},
	morecomment=[s]{"""}{"""},
	commentstyle=\color{commentcolour}\slshape,
	emph={array, matmul, transpose, float32},
	emphstyle=[4]\color{numpycolour},
	emph={[5]assert,yield},
	emphstyle=[5]\color{keywordcolour}\bfseries,
	emph={[6]range},
	emphstyle={[6]\color{keywordcolour}\bfseries},
	literate=*%
	{:}{{\literatecolour:}}{1}%
	{=}{{\literatecolour=}}{1}%
	{-}{{\literatecolour-}}{1}%
	{+}{{\literatecolour+}}{1}%
	{*}{{\literatecolour*}}{1}%
	{**}{{\literatecolour{**}}}2%
	{/}{{\literatecolour/}}{1}%
	{//}{{\literatecolour{//}}}2%
	{!}{{\literatecolour!}}{1}%
	{<}{{\literatecolour<}}{1}%
	{>}{{\literatecolour>}}{1}%
	{>>>}{\pythonprompt}{3},
	frame=trbl,
	rulecolor=\color{black!40},
	backgroundcolor=\color{gray!5},
	breakindent=.5\textwidth,
	frame=single,
	breaklines=true,
	basicstyle=\ttfamily\footnotesize,%
	keywordstyle=\color{keywordcolour},%
	emphstyle={[7]\color{keywordcolour}},%
	emphstyle=\color{exceptioncolour},%
	literate=*%
	{:}{{\literatecolour:}}{2}%
	{=}{{\literatecolour=}}{2}%
	{-}{{\literatecolour-}}{2}%
	{+}{{\literatecolour+}}{2}%
	{*}{{\literatecolour*}}2%
	{**}{{\literatecolour{**}}}3%
	{/}{{\literatecolour/}}{2}%
	{//}{{\literatecolour{//}}}{2}%
	{!}{{\literatecolour!}}{2}%
	{<}{{\literatecolour<}}{2}%
	{<=}{{\literatecolour{<=}}}3%
	{>}{{\literatecolour>}}{2}%
	{>=}{{\literatecolour{>=}}}3%
	{==}{{\literatecolour{==}}}3%
	{!=}{{\literatecolour{!=}}}3%
	{+=}{{\literatecolour{+=}}}3%
	{-=}{{\literatecolour{-=}}}3%
	{*=}{{\literatecolour{*=}}}3%
	{/=}{{\literatecolour{/=}}}3%
}

\lstnewenvironment{python}
{\lstset{style=python}}
{}

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm

\newenvironment{sciabstract}{%
	\begin{quote} \bf}
	{\end{quote}}


\renewcommand\refname{References and Notes}

\newcounter{problem}
\newcounter{solution}

\newcommand\Problem{%
	\stepcounter{problem}%
	\textbf{\theproblem.}~%
	\setcounter{solution}{0}%
}

\newcommand\Solution{%
	\textbf{Solution:}\\%
}

\newcommand\ASolution{%
	\stepcounter{solution}%
	\textbf{Solution \solution:}\\%
}

\parindent 0in
\parskip 1em

\newcounter{lastnote}
\newenvironment{scilastnote}{%
	\setcounter{lastnote}{\value{enumiv}}%
	\addtocounter{lastnote}{+1}%
	\begin{list}%
		{\arabic{lastnote}.}
		{\setlength{\leftmargin}{.22in}}
		{\setlength{\labelsep}{.5em}}}
	{\end{list}}

\title{Deep Learning Lab \\ \Large{Assignment 1: Neural Networks} \\[0.3em] \normalsize{Faculty of Informatics} \\ \normalsize{Universit√† della Svizzera Italiana}}


\author {{Giorgia Adorni}	\\ \normalsize{giorgia.adorni@usi.ch}}


\date{\today}

\bibliography{scibib}

%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%

\begin{document} 
	
	% Double-space the manuscript.
	%\baselineskip24pt
	
	\maketitle 
	
	Consider the polynomial $p$ given by
	\begin{equation*}
	 p(x)=x^3+2x^2-4x-8=\sum_{i=1}^4 w_i^*x^{i-1} \mbox{,}
	\end{equation*} 
	
	where $\textbf{w}^*=[-8,-4,2,1]^T$.
	
	Consider also an iid dataset $\mathcal{D} = \{(x_i, y_i)\}^N_{i=1}$, where $y_i = p(x_i)+\epsilon_i$, and each $\epsilon_i$ is drawn from a normal distribution with mean zero and standard deviation $\sigma = \frac{1}{2}$.
	
	If the vector $\textbf{w}^*$ were unknown, linear regression could estimate it given the dataset $\mathcal{D}$. This would require applying a feature map to transform the
	original dataset $\mathcal{D}$ into an expanded dataset $\mathcal{D}'= \{(x_i, y_i)\}^N_{i=1}$ , where $x_i = [1,x_i,x_i^2,x_i^3]$.
	
	%%%%%
	\section{Introduction}
	The scope of this {project} is to perform polynomial regression using a dataset $\mathcal{D}'$, in particular finding an estimate of $\textbf{w}^*=[-8,-4,2,1]^T$ supposing that such vector is unknown.\\
	An interval $[-3, 2]$ for $x_i$, a sample of size $100$ created with a seed of $0$ for training, and a sample of size $100$ created with a seed of $1$ for validation, and $\sigma = \frac{1}{2}$ were assumed.
	
	\section{Tuning the learning rate}
	The learning rate is a configurable hyper-parameter that represent the positive amount of which weights are updated during the training of a neural network.\\
	I tried to discover a suitable learning rate via trial, setting the initial number of iterations to $2000$.
	In the first test I set the learning rate to a traditional default value of $0.1$. In this case, the value is too high that the algorithm diverges.\\
	Hence, I choose to decrease the value to $0.01$ obtaining a validation loss of $0.22$, that is a good results for the gradient descent.
		
	\section{Iterations and early stopping}
	Fixed the learning rate, I decided to use the \textbf{early stopping} rule in order to abort the training procedure when the performance on the validation set stops to improving and therefore avoid over-fitting. \\
	In particular, I measured the validation loss in each iteration, keeping track of the lowest one, and I stopped the training when the validation loss has not improved, compared to the best, after $10$ steps.\\
	In this examining case, after $1248$ iterations the model reach the best loss of $0.2177$.
	
	\section{Loss}
	
	I used {TensorBoard} to display the loss curves as functions of the gradient descent iterations, for both the training and validation set (over time (iterations)), which are shown in the Figure \ref{fig:model1-loss}.

	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.5\linewidth]{../src/img/model1-es-loss-label.jpg}
		\captionof{figure}{model1-loss}
		\label{fig:model1-loss}
	\end{figure}
	 
	The two training and validation curves start at $29.69$ and $29.10$ respectively, and go down to $0.24$ and $0.22$ at the $1248th$ iteration.
	It is therefore seen that the validation loss is lower than the training loss. The reason why this happens is that the training loss is measured during each epoch while validation loss is computed at the end of the learning phase of the same epoch.

		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model1-polynomial.png}
			\captionof{figure}{model1-polynomial}
			\label{fig:model1-polynomial}
		\end{minipage}
	\end{figure}

		\begin{figure}
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model2-dataset.png}
			\captionof{figure}{model2-dataset}
			\label{fig:model2-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model2-polynomial.png}
			\captionof{figure}{model2-polynomial}
			\label{fig:model2-polynomial}
		\end{minipage}
	\end{figure}

		\begin{figure}
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model3-dataset.png}
			\captionof{figure}{model3-dataset}
			\label{fig:model3-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model3-polynomial.png}
			\captionof{figure}{model3-polynomial}
			\label{fig:model3-polynomial}
		\end{minipage}
	\end{figure}

		\begin{figure}
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model4-dataset.png}
			\captionof{figure}{model4-dataset}
			\label{fig:model4-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model4-polynomial.png}
			\captionof{figure}{model4-polynomial}
			\label{fig:model4-polynomial}
		\end{minipage}
	\end{figure}

		\begin{figure}
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model5-dataset.png}
			\captionof{figure}{model5-dataset}
			\label{fig:model5-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model5-polynomial.png}
			\captionof{figure}{model5-polynomial}
			\label{fig:model5-polynomial}
		\end{minipage}
	\end{figure}

		\begin{figure}
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model6-dataset.png}
			\captionof{figure}{model6-dataset}
			\label{fig:model6-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model6-polynomial.png}
			\captionof{figure}{model6-polynomial}
			\label{fig:model6-polynomial}
		\end{minipage}
	\end{figure}

		\begin{figure}
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model7-dataset.png}
			\captionof{figure}{model7-dataset}
			\label{fig:model7-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model7-polynomial.png}
			\captionof{figure}{model7-polynomial}
			\label{fig:model7-polynomial}
		\end{minipage}
	\end{figure}

	\begin{figure}
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model8-dataset.png}
			\captionof{figure}{model8-dataset}
			\label{fig:model8-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model8-polynomial.png}
			\captionof{figure}{model8-polynomial}
			\label{fig:model8-polynomial}
		\end{minipage}
	\end{figure}

	
\end{document}




















