\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one 
%cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\pagestyle{fancy}

\setlength\parindent{24pt}

\fancyhf{}

\lhead{\footnotesize Deep Learning Lab: Assignment 1}

\rhead{\footnotesize Giorgia Adorni}

\cfoot{\footnotesize \thepage} 

\begin{document}
	

	\thispagestyle{empty}  
	\noindent{
	\begin{tabular}{p{15cm}} 
		{\large \bf Deep Learning Lab} \\
		Universit√† della Svizzera Italiana \\ Faculty of Informatics \\ \today  \\
		\hline
		\\
	\end{tabular} 
	
	\vspace*{0.3cm} 
	
	\begin{center}
		{\Large \bf Assignment 1: Polynomial Regression}
		\vspace{2mm}
		
		{\bf Giorgia Adorni (giorgia.adorni@usi.ch)}
		
	\end{center}  
}
	\vspace{0.4cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\noindent {Consider the polynomial $p$ given by}
	\begin{equation*}
	p(x)=x^3+2x^2-4x-8=\sum_{i=1}^4 w_i^*x^{i-1} \mbox{,}
	\end{equation*} 
	where $\textbf{w}^*=[-8,-4,2,1]^T$.
	
	Consider also an iid dataset $\mathcal{D} = \{(x_i, y_i)\}^N_{i=1}$, where 
	$y_i = p(x_i)+\epsilon_i$, and each $\epsilon_i$ is drawn from a normal 
	distribution with mean zero and standard deviation $\sigma = \frac{1}{2}$.
	
	If the vector $\textbf{w}^*$ were unknown, linear regression could estimate it given the dataset $\mathcal{D}$. This would require applying a feature map to transform the
	original dataset $\mathcal{D}$ into an expanded dataset $\mathcal{D}'= \{(x_i, y_i)\}^N_{i=1}$ , where $x_i = [1,x_i,x_i^2,x_i^3]$.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Introduction}
	The scope of this {project} is to perform polynomial regression using a dataset $\mathcal{D}'$, in particular finding an estimate of $\textbf{w}^*=[-8,-4,2,1]^T$ supposing that such vector is unknown.\\
	An interval $[-3, 2]$ for $x_i$, a sample of size $100$ created with a seed of $0$ for training, and a sample of size $100$ created with a seed of $1$ for validation, and $\sigma = \frac{1}{2}$ were assumed.
	
	\section{Tuning the Learning Rate}
	The learning rate is a configurable hyper-parameter that represents the 
	scaling factor of the gradient by which weights are updated during gradient 
	descent.\\
	I tried to discover a suitable learning rate via trial and error, setting 
	the initial number of iterations to $2000$, a relatively high value for the 
	amount of parameters present.
	In the first test, I set the learning rate to a traditional default value 
	of $0.1$. In this case, the value is so high that the algorithm diverges.\\
	Hence, I chose to decrease the value to $0.01$ obtaining a validation loss 
	of $0.22$, that is a good enough result for the purpose of this assignment.
	
	\section{Iterations and Early Stopping}
	Fixed the learning rate, I decided to reduce the number of iterations using 
	the \textit{early stopping} technique. This rule can to abort the 
	training procedure when the performance on the validation set stops 
	improving and therefore it avoids overfitting. \\
	In particular, I measured the validation loss after each iteration, keeping 
	track of the lowest one, and I stopped the training when the validation 
	loss did not improve, compared to the best, after $10$ steps.\\
	In this case, after $1248$ iterations the model reached the best loss of 
	$0.2177$.
	
	\section{Loss}
	I used {TensorBoard} to display the loss curve as a function of the 
	gradient 
	descent iterations, for both the training and validation set, which are 
	shown in Figure \ref{fig:model1-loss}.
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.65\linewidth]{../src/img/loss/model1-es-loss.jpg}
		\captionof{figure}{Training and validation loss}
		\label{fig:model1-loss}
	\end{figure}
	
	The two training and validation curves start at $29.69$ and $29.10$ 
	respectively and go down to $0.24$ and $0.22$ at the $1248\mathrm{th}$ 
	iteration.
	It is therefore seen that the validation loss is lower than the training 
	loss. {\color{red}The reason why this happens is that the training loss is 
	measured during each epoch while validation loss is computed at the end of 
	the learning phase of the same epoch.}
	
	\section{Polynomial Regression} 
	Considered the polynomial defined by $\textbf{w}^*=[-8,-4,2,1]^T$, Figure 
	\ref{fig:model1-dataset} visualises the training and the validation 
	datasets generated, while Figure \ref{fig:model1-polynomial}, shows 
	the polynomial defined by $\textbf{w}^*$ and the polynomial estimated 
	$\hat{\textbf{w}}$.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model1-dataset.png}
			\caption{Training and validation datasets}
			\label{fig:model1-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model1-polynomial.png}
			\caption{True and estimated polynomials}
			\label{fig:model1-polynomial}
		\end{minipage}
	\end{figure}
	
 	The true polynomial defined by the coefficients 
 	$\textbf{w}^*=[-8,-4,2,1]^T$ and the one that has been estimated 
 	$\hat{\textbf{w}}=[-7.86,-4.08,1.98,1.01]^T$ are very close.
 	From the plot in Figure \ref{fig:model1-polynomial+dataset} it is clear 
 	that the estimated polynomial curve is fitting well enough the data.
 	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\linewidth]{../src/img/model1-polynomial+dataset.png}
		\caption{Polynomial curves and datasets}
		\label{fig:model1-polynomial+dataset}
	\end{figure}
	
	\section{Training Set Reduction}
	It is a fact that the behaviour of the model depends on the number of 
	samples in te training set.
	
	From the below pictures we can observe that reducing the training dataset 
	from $100$ to $50$, $10$, and finally $5$ observations, the model is not 
	able to understand the pattern in the set anymore.  \\
	While with $50$ observations the loss curve shows that the model continues 
	performs well. \\
	By reducing the number of samples down to $10$, the validation loss starts 
	fluctuating but with a tendency to decrease.\\
	Finally, reducing the number of observations to $5$ the model overfits.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model2-loss.jpg}
			%\caption*{model2-loss}
			%\label{fig:model2-loss}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model2-polynomial+dataset.png}
			%\caption*{model2-polynomial+dataset}
			%\label{fig:model2-polynomial+dataset}
		\end{minipage}
	\caption{Loss and polynomial curve for training dataset with $50$ 
	observations}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model3-loss.jpg}
			%\caption*{model3-loss}
			%\label{fig:model3-loss}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model3-polynomial+dataset.png}
			%\caption*{model3-polynomial+dataset}
			%\label{fig:model3-polynomial+dataset}
		\end{minipage}
	\caption{Loss and polynomial curve for training dataset with $10$ 
		observations}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model4-loss.jpg}
			%\caption*{model4-loss}
			%\label{fig:model4-loss}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model4-polynomial+dataset.png}
			%\caption*{model4-polynomial+dataset}
			%\label{fig:model4-polynomial+dataset}
		\end{minipage}
	\caption{Loss and polynomial curve for training dataset with $5$ 
	observations}
	\end{figure}
	
	\section{Sigma Increase}
	From the below pictures we can observe that if we further increase the  
	standard deviation, during the creation of the dataset, from $0.5$ to $2$, 
	$4$, and finally $8$, the model performance worsen a lot. The losses 
	increase respectively to $3.48$, $13.93$ and $55.74$. Moreover, the 
	gap between training and validation loss growth.
	One solution to improve the performance could be to increase the 
	observations in the training set.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model5-loss.jpg}
			%\caption*{model5-loss}
			%\label{fig:model5-loss}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model5-polynomial+dataset.png}
			%\caption*{model5-polynomial+dataset}
			%\label{fig:model5-polynomial+dataset}
		\end{minipage}
		\caption{Loss and polynomial curves for training dataset with sigma $2$}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model6-loss.jpg}
			%\caption*{model6-loss}
			%\label{fig:model6-loss}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model6-polynomial+dataset.png}
			%\caption*{model6-polynomial+dataset}
			%\label{fig:model6-polynomial+dataset}
		\end{minipage}
	\caption{Loss and polynomial curves for training dataset with sigma $4$}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model7-loss.jpg}
			%\caption*{model7-loss}
			%\label{fig:model7-loss}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model7-polynomial+dataset.png}
			%\caption*{model7-polynomial+dataset}
			%\label{fig:model7-polynomial+dataset}
		\end{minipage}
	\caption{Loss and polynomial curves for training dataset with sigma $8$}
	\end{figure}
	
	
	\section{Higher-degree Polynomial}
	The last experiment consists in reducing the training dataset to $10$ 
	observations and comparing a fitted polynomial of degree three with 
	one of degree four.
	The results are visualised in Figure \ref*{fig:model8}.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model8-loss.jpg}
			%\caption{Loss of a polynomial of $4\mathrm{th}$ degree}
			%\label{fig:model8-loss}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model8-polynomial+dataset.png}
			%\caption{Polynomial curves of $4\mathrm{th}$ degree}
			%\label{fig:model8-polynomial+dataset}
		\end{minipage}
	\caption{Polynomial of 4th degree}
	\label{fig:model8}
	\end{figure}

	For what concerns the loss, the validation loss continues to grow in the 
	first $400$ iterations and after that starts going down, while the 
	training loss is constantly decreasing.
	In Figure \ref*{fig:model8-test} is clear that the estimated 
	polynomial of $4\mathrm{th}$ degree is capable of fit well the data in 
	the interval $[-1,2]$ while the polynomial of $3\mathrm{rd}$ degree 
	in the interval $[-1.5,1]$.\bigskip
	
	Increasing the number of observations in the training set up to $8000$, it 
	is possible to have a good understanding of the validation loss behaviour 
	over time. 
	
	\begin{figure}[H]
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=0.85\linewidth]{../src/img/loss/model8-loss-test.jpg}
			%\caption*{model8-loss-test}
			%\label{fig:model8-loss-test}
		\end{minipage}
		~
		\begin{minipage}[c]{.49\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model8-test-polynomial+dataset.png}
			%\caption*{model8-test-polynomial+dataset}
			%\label{fig:model8-test-polynomial+dataset}
		\end{minipage}
	\caption{Polynomial of 4th degree over $8000$ iterations}
	\label{fig:model8-test}
	\end{figure}

\end{document}
