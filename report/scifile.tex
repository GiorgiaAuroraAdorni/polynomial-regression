\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\pagestyle{fancy}

\fancyhf{}

\lhead{\footnotesize Deep Learning Lab: Assigment 1}

\rhead{\footnotesize Giorgia Adorni)}

\cfoot{\footnotesize \thepage} 

\begin{document}
	

	\thispagestyle{empty}  
	
	\begin{tabular}{p{15.5cm}} 
		{\large \bf Deep Learning Lab} \\
		Universit√† della Svizzera Italiana \\ Faculty of Informatics \\ \today  \\
		\hline
		\\
	\end{tabular} 
	
	\vspace*{0.3cm} 
	
	\begin{center}
		{\Large \bf Assignment 1: Neural Networks}
		\vspace{2mm}
		
		{\bf Giorgia Adorni (giorgia.adorni@usi.ch)}
		
	\end{center}  
	
	\vspace{0.4cm}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	% Up until this point you only have to make minor changes for every week (Number of the homework). Your write up essentially starts here.
	
	Consider the polynomial $p$ given by
	\begin{equation*}
	p(x)=x^3+2x^2-4x-8=\sum_{i=1}^4 w_i^*x^{i-1} \mbox{,}
	\end{equation*} 
	
	where $\textbf{w}^*=[-8,-4,2,1]^T$.
	
	Consider also an iid dataset $\mathcal{D} = \{(x_i, y_i)\}^N_{i=1}$, where $y_i = p(x_i)+\epsilon_i$, and each $\epsilon_i$ is drawn from a normal distribution with mean zero and standard deviation $\sigma = \frac{1}{2}$.
	
	If the vector $\textbf{w}^*$ were unknown, linear regression could estimate it given the dataset $\mathcal{D}$. This would require applying a feature map to transform the
	original dataset $\mathcal{D}$ into an expanded dataset $\mathcal{D}'= \{(x_i, y_i)\}^N_{i=1}$ , where $x_i = [1,x_i,x_i^2,x_i^3]$.
	
	%%%%%
	\section{Introduction}
	The scope of this {project} is to perform polynomial regression using a dataset $\mathcal{D}'$, in particular finding an estimate of $\textbf{w}^*=[-8,-4,2,1]^T$ supposing that such vector is unknown.\\
	An interval $[-3, 2]$ for $x_i$, a sample of size $100$ created with a seed of $0$ for training, and a sample of size $100$ created with a seed of $1$ for validation, and $\sigma = \frac{1}{2}$ were assumed.
	
	\section{Tuning the learning rate}
	The learning rate is a configurable hyper-parameter that represent the positive amount of which weights are updated during the training of a neural network.\\
	I tried to discover a suitable learning rate via trial, setting the initial number of iterations to $2000$.
	In the first test I set the learning rate to a traditional default value of $0.1$. In this case, the value is too high that the algorithm diverges.\\
	Hence, I choose to decrease the value to $0.01$ obtaining a validation loss of $0.22$, that is a good results for the gradient descent.
	
	\section{Iterations and early stopping}
	Fixed the learning rate, I decided to use the \textbf{early stopping} rule in order to abort the training procedure when the performance on the validation set stops to improving and therefore avoid over-fitting. \\
	In particular, I measured the validation loss in each iteration, keeping track of the lowest one, and I stopped the training when the validation loss has not improved, compared to the best, after $10$ steps.\\
	In this examining case, after $1248$ iterations the model reach the best loss of $0.2177$.
	
	\section{Loss}
	
	I used {TensorBoard} to display the loss curves as functions of the gradient descent iterations, for both the training and validation set (over time (iterations)), which are shown in the Figure \ref{fig:model1-loss}.
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.5\linewidth]{../src/img/loss/model1-es-loss-label.jpg}
		\captionof{figure}{model1-loss}
		\label{fig:model1-loss}
	\end{figure}
	
	The two training and validation curves start at $29.69$ and $29.10$ respectively, and go down to $0.24$ and $0.22$ at the $1248\mathrm{th}$ iteration.
	It is therefore seen that the validation loss is lower than the training loss. The reason why this happens is that the training loss is measured during each epoch while validation loss is computed at the end of the learning phase of the same epoch.
	
	\section{Polynomial defined by w\_star and the polynomial defined by your estimate w\_hat} 
	Considered the polynomial defined by $\textbf{w}^*=[-8,-4,2,1]^T$, in the Figure \ref{fig:model1-dataset} are visualised the training and the validation datasets. 
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{../src/img/model1-dataset.png}
		\captionof{figure}{model1-dataset}
		\label{fig:model1-dataset}
	\end{figure}
	
	Since the .... the polynomial defined by $\textbf{w}^*$ and the polynomial estimated $\hat{\textbf{w}}$ are very close, as we can see in the Figure \ref{fig:model1-polynomial}.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\linewidth]{../src/img/model1-polynomial.png}
		\captionof{figure}{model1-polynomial}
		\label{fig:model1-polynomial}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\linewidth]{../src/img/model1-polynomial+dataset.png}
		\captionof{figure}{model1-polynomial+dataset}
		\label{fig:model1-polynomial+dataset}
	\end{figure}
	
	to visualize 
	to plot
	
	\section{Experiment: training set reduction}
	
	Document what happens when the training dataset is reduced to 50, 10, and 5 observations
	
	\begin{figure}[H]
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model2-dataset.png}
			\captionof{figure}{model2-dataset}
			\label{fig:model2-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model2-polynomial.png}
			\captionof{figure}{model2-polynomial}
			\label{fig:model2-polynomial}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model3-dataset.png}
			\captionof{figure}{model3-dataset}
			\label{fig:model3-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model3-polynomial.png}
			\captionof{figure}{model3-polynomial}
			\label{fig:model3-polynomial}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model4-dataset.png}
			\captionof{figure}{model4-dataset}
			\label{fig:model4-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model4-polynomial.png}
			\captionof{figure}{model4-polynomial}
			\label{fig:model4-polynomial}
		\end{minipage}
	\end{figure}
	
	\section{Experiment: sigma increasing}
	
	Document what happens when $\sigma$ is increased to 2, 4 and 8
	
	\begin{figure}[H]
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model5-dataset.png}
			\captionof{figure}{model5-dataset}
			\label{fig:model5-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model5-polynomial.png}
			\captionof{figure}{model5-polynomial}
			\label{fig:model5-polynomial}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model6-dataset.png}
			\captionof{figure}{model6-dataset}
			\label{fig:model6-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model6-polynomial.png}
			\captionof{figure}{model6-polynomial}
			\label{fig:model6-polynomial}
		\end{minipage}
	\end{figure}
	
	\begin{figure}[H]
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model7-dataset.png}
			\captionof{figure}{model7-dataset}
			\label{fig:model7-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model7-polynomial.png}
			\captionof{figure}{model7-polynomial}
			\label{fig:model7-polynomial}
		\end{minipage}
	\end{figure}
	
	
	\section{Experiment: polynomial of degree four}
	Reduce your training dataset to 10 observations, and compare fitting a polynomial of degree three (as before) with fitting a polynomial of degree four (which does not match the underlying polynomial p). Plot the resulting polynomials and document the validation loss.
	
	\begin{figure}[H]
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model8-dataset.png}
			\captionof{figure}{model8-dataset}
			\label{fig:model8-dataset}
		\end{minipage}
		~
		\begin{minipage}[c]{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{../src/img/model8-polynomial.png}
			\captionof{figure}{model8-polynomial}
			\label{fig:model8-polynomial}
		\end{minipage}
	\end{figure}
	
	
\end{document}